# Agent Concepts: MCP, FastMCP & Hybrid Architecture

This document is the canonical reference for the key architectural concepts
used in the Snowflake BI Agent.  Engineers new to the codebase should read
this before diving into the code.

---

## 1. Model Context Protocol (MCP)

**MCP** is an open standard (introduced by Anthropic, adopted across the
industry) that defines how AI models communicate with external tools and
data sources.

### Why it matters

Without a protocol, every agent/tool integration is bespoke: you write custom
glue code to call your function, serialise arguments, deserialise results, and
handle errors — for every tool and every LLM.

MCP standardises this:

```
User question
    ↓
LLM (Gemini)        ← knows tools via FunctionDeclaration (JSON Schema)
    ↓
MCP server          ← hosts the tool implementations
    ↓
Tool function       ← plain Python
    ↓
Result (string) → back to LLM → answer
```

**In this agent**, the MCP server is the `FastMCP` instance in
`agent/tool_definitions/registry.py`.

---

## 2. FastMCP

**FastMCP** is a Python library that implements the MCP server layer with
minimal boilerplate.

### Compared to raw MCP

| Without FastMCP | With FastMCP |
|---|---|
| Write JSON Schema for every parameter | Inferred automatically from Python type hints |
| Register tools manually | `@mcp.tool()` decorator does it |
| Handle serialisation | Automatic |
| Manage server lifecycle | Handled by `FastMCP` |

### The server singleton

```python
# agent/tool_definitions/registry.py
from fastmcp import FastMCP

mcp = FastMCP("AI Agent")
```

One `FastMCP` instance, shared across all tool modules.  Each tool module
imports `mcp` and registers on it via `@mcp.tool()`.

---

## 3. The `@mcp.tool()` Decorator

This is how a plain Python function becomes an LLM-callable tool:

```python
@mcp.tool()
async def get_account_info(account_name: str) -> str:
    """Look up information about a specific customer or account.
    
    Use this when the user asks about a specific account, customer, or client
    by name. Returns key details and metrics for that account.
    """
    ...
```

When the decorator runs at import time, FastMCP:

1. **Reads the function name** → tool name (`get_account_info`)
2. **Reads the docstring** → tool description (what the LLM sees)
3. **Reads the type hints** → builds the JSON Schema for parameters
4. **Stores everything** in the internal tool registry

Later, `agent/core/mcp_registry.py` converts this registry into Gemini
`FunctionDeclaration` objects so the LLM can discover and invoke the tools.

### Docstring quality = LLM accuracy

The tool docstring is the LLM's only instruction for *when* and *how* to
call the tool.  A vague docstring leads to wrong tool selection.

```python
# ❌ Too vague
"""Query data."""

# ✅ Tells the LLM exactly when to use it
"""[INTERNAL] Execute a Snowflake SQL query and return results.
Business users should not call this directly. The agent uses this
internally to answer exploratory, ad-hoc questions.
"""
```

---

## 4. Hybrid Architecture: Deterministic + Generative Tools

The agent deliberately uses **two complementary strategies** for answering
data questions.  Both live in `agent/tool_definitions/query_tools.py`.

### Strategy 1 — Deterministic (Hardcoded SQL)

**Tool:** `get_account_info(account_name)`

```python
query = (
    f"SELECT * FROM FINANCIALS.PUBLIC.FINANCIAL_SUMMARY "
    f"WHERE ACCOUNT_NAME ILIKE '%{account_name}%' LIMIT 1"
)
```

| Property | Detail |
|---|---|
| SQL source | Written by a human engineer |
| LLM role | Supply the *parameter* only |
| Reliability | 100% — same input, same query, every time |
| Flexibility | Low — adding columns requires a code change |
| Best for | High-frequency, mission-critical, SLA-sensitive lookups |

### Strategy 2 — Generative (Text-to-SQL)

**Tool:** `_query_data_internal(query)`

```python
# The LLM generates the entire SQL at runtime:
query = "SELECT customer, SUM(revenue) FROM ..."  # ← LLM wrote this
results = toolkit.snowflake.query(query)
```

| Property | Detail |
|---|---|
| SQL source | Generated by Gemini from natural language |
| LLM role | Write the entire SQL |
| Reliability | Medium — LLM can make schema errors |
| Flexibility | High — answers any question |
| Best for | Exploratory, ad-hoc, one-off analytical queries |

### Why both?

> A BI agent with only hardcoded SQL is brittle — every new question
> requires a developer sprint.  A BI agent with only Text-to-SQL is
> risky — critical SLA lookups may fail due to LLM hallucinations or
> schema drift.

The hybrid gives us **stability where it matters, flexibility everywhere else**.

| Type | Tool | When the LLM picks it |
|---|---|---|
| Deterministic | `get_account_info` | User asks about a specific account by name |
| Generative | `_query_data_internal` | Any other data question |

---

## 5. Dependency Injection with Toolkit

All infrastructure clients (Snowflake, Sheets, ResourceManager,
ErrorHandler) are composed in `agent/tools/toolkit.py`'s `Toolkit` class:

```python
toolkit = Toolkit(Config.from_env())
# toolkit.snowflake  → SnowflakeClient
# toolkit.sheets     → SheetsClient
# toolkit.resources  → ResourceManager
# toolkit.error_handler → ErrorHandler
```

Each tool module uses a **lazy singleton**:

```python
_toolkit: Toolkit | None = None

def get_toolkit() -> Toolkit:
    global _toolkit
    if _toolkit is None:
        _toolkit = Toolkit(Config.from_env())
    return _toolkit
```

This guarantees exactly **one Snowflake connection** per agent session,
regardless of how many tool calls are made, and makes clients easy to
mock in tests.
